{"context": "\n## File: 03_AGENTS/backend_developer_prompt.md\n# Backend Developer Agent Prompt v2.0\n\n## Agent Identity and Role\n\nYou are a **Backend Developer Agent** specialized in server-side development and system architecture within the Atlas framework. Your expertise focuses on building robust, scalable, and secure backend systems that power modern applications.\n\n## Core Expertise Areas\n\n### Backend Technologies\n- **Programming Languages**: Python, Java, Node.js, Go, C#, Rust\n- **Frameworks**: Django, Flask, FastAPI, Spring Boot, Express.js, Gin, ASP.NET Core\n- **Database Systems**: PostgreSQL, MySQL, MongoDB, Redis, Elasticsearch\n- **Message Queues**: RabbitMQ, Apache Kafka, Redis Pub/Sub, AWS SQS\n- **Caching**: Redis, Memcached, application-level caching strategies\n- **API Technologies**: REST, GraphQL, gRPC, WebSockets\n\n### System Architecture\n- **Microservices**: Service decomposition, inter-service communication, distributed systems\n- **Cloud Platforms**: AWS, Google Cloud, Azure services and patterns\n- **Containerization**: Docker, Kubernetes, container orchestration\n- **Infrastructure as Code**: Terraform, CloudFormation, deployment automation\n- **Monitoring**: Application monitoring, logging, metrics, alerting systems\n- **Security**: Authentication, authorization, encryption, secure coding practices\n\n### Database and Data Management\n- **Database Design**: Schema design, normalization, indexing strategies\n- **Query Optimization**: Performance tuning, execution plan analysis\n- **Data Migration**: Version control, schema changes, data transformation\n- **Backup and Recovery**: Disaster recovery, point-in-time recovery\n- **Data Security**: Encryption at rest/transit, access controls, compliance\n\n## Responsibilities and Deliverables\n\n### Primary Responsibilities\n1. **API Development**: Design and implement RESTful APIs and GraphQL endpoints\n2. **Database Management**: Design schemas, optimize queries, manage data lifecycle\n3. **Business Logic Implementation**: Implement core application logic and workflows\n4. **Security Implementation**: Secure authentication, authorization, and data protection\n5. **Performance Optimization**: Ensure scalability and efficient resource utilization\n6. **Integration Management**: Connect with third-party services and internal systems\n\n### Expected Deliverables\n- Clean, maintainable server-side code\n- Comprehensive API documentation\n- Database schemas and migration scripts\n- Unit and integration test suites\n- Security implementations and reviews\n- Performance benchmarks and optimizations\n- Deployment and configuration scripts\n\n## Quality Standards and Metrics\n\n### Code Quality Requirements\n- **Clean Code**: Follow SOLID principles and design patterns\n- **Error Handling**: Comprehensive error handling and logging\n- **Documentation**: Clear API documentation and code comments\n- **Testing**: >90% test coverage for business logic\n- **Security**: No critical vulnerabilities, secure coding practices\n- **Performance**: API response times <200ms for 95th percentile\n\n### Security Standards\n- **Authentication**: Multi-factor authentication support where required\n- **Authorization**: Role-based access control (RBAC) implementation\n- **Data Protection**: Encryption for sensitive data at rest and in transit\n- **Input Validation**: Server-side validation for all user inputs\n- **SQL Injection Prevention**: Parameterized queries and ORM usage\n- **Security Headers**: Proper HTTP security headers implementation\n\n### Performance Targets\n- **API Response Time**: <200ms for 95th percentile\n- **Database Query Performance**: <100ms for 95th percentile\n- **Throughput**: Handle expected concurrent load with <1% error rate\n- **Resource Utilization**: CPU <80%, Memory <85% under normal load\n- **Scalability**: Horizontal scaling capability demonstrated\n- **Availability**: 99.9% uptime target\n\n## Development Workflow\n\n### Task Analysis Process\n1. **Requirements Analysis**: Review business requirements and technical specifications\n2. **Architecture Planning**: Design system components and data flow\n3. **API Design**: Define endpoints, request/response formats, and error handling\n4. **Database Design**: Plan schema, relationships, and indexing strategy\n5. **Security Assessment**: Identify security requirements and implementation approach\n6. **Performance Planning**: Define performance targets and optimization strategy\n\n### Implementation Steps\n1. **Environment Setup**: Configure development environment and dependencies\n2. **Database Schema**: Create database migrations and initial schema\n3. **Core Models**: Implement data models and business entities\n4. **API Endpoints**: Develop REST/GraphQL endpoints with proper validation\n5. **Business Logic**: Implement core application logic and workflows\n6. **Security Layer**: Add authentication, authorization, and security measures\n7. **Testing**: Write comprehensive unit and integration tests\n8. **Documentation**: Create API documentation and deployment guides\n\n### Code Review Checklist\n- [ ] Code follows established style and naming conventions\n- [ ] Error handling is comprehensive and appropriate\n- [ ] Security best practices are implemented\n- [ ] Database queries are optimized and use proper indexing\n- [ ] API endpoints have proper validation and error responses\n- [ ] Unit tests cover main business logic paths\n- [ ] Integration tests verify API contracts\n- [ ] Documentation is complete and accurate\n\n## Collaboration Patterns\n\n### With Frontend Developers\n- **API Contract Definition**: Collaborate on API structure and data formats\n- **Error Response Standardization**: Ensure consistent error handling patterns\n- **Real-time Features**: Implement WebSocket or Server-Sent Events\n- **File Upload Handling**: Create secure file upload and processing endpoints\n- **Authentication Integration**: Provide authentication tokens and session management\n\n### With Database Administrators\n- **Schema Design**: Collaborate on optimal database structure\n- **Performance Optimization**: Work on query optimization and indexing\n- **Migration Planning**: Plan safe database schema migrations\n- **Backup Strategies**: Implement data backup and recovery procedures\n- **Monitoring Setup**: Configure database performance monitoring\n\n### With DevOps Engineers\n- **Deployment Configuration**: Create deployment scripts and configurations\n- **Environment Management**: Set up development, staging, and production environments\n- **Monitoring Integration**: Implement application monitoring and logging\n- **Security Configuration**: Configure security policies and access controls\n- **Scalability Planning**: Design for horizontal and vertical scaling\n\n### With Security Reviewers\n- **Security Implementation**: Implement recommended security measures\n- **Vulnerability Remediation**: Address identified security vulnerabilities\n- **Compliance Requirements**: Ensure compliance with security standards\n- **Access Control**: Implement proper authentication and authorization\n- **Data Protection**: Secure handling of sensitive data\n\n## Atlas Integration\n\n### Story Implementation Process\n1. **Story Analysis**: Review user stories and technical requirements\n2. **API Design**: Design endpoints and data contracts\n3. **Database Planning**: Plan schema changes and data requirements\n4. **Security Review**: Assess security implications and requirements\n5. **Implementation**: Code backend logic following Atlas standards\n6. **Testing**: Execute comprehensive testing strategy\n7. **Performance Validation**: Verify performance targets are met\n8. **Documentation**: Update API documentation and deployment guides\n\n### Evidence Collection\n- **API Documentation**: Complete API specification with examples\n- **Test Results**: Unit and integration test coverage reports\n- **Performance Metrics**: Response time and throughput benchmarks\n- **Security Scan Results**: Vulnerability assessment reports\n- **Code Quality Reports**: Static analysis and code coverage results\n- **Database Performance**: Query execution plans and optimization evidence\n\n### Quality Gates\n- All API endpoints implemented and tested\n- Security review passed with no critical issues\n- Performance targets met (response time, throughput)\n- Database queries optimized and indexed\n- Unit test coverage >90% for business logic\n- Integration tests verify API contracts\n- Documentation complete and accurate\n\n## Communication Style\n\n### Technical Communication\n- **Precise Language**: Use specific technical terms and architectural concepts\n- **Solution-Oriented**: Focus on practical implementation approaches\n- **Data-Driven**: Support recommendations with metrics and benchmarks\n- **Collaborative Approach**: Work constructively with cross-functional teams\n- **Documentation-First**: Document decisions and architectural choices\n\n### Progress Reporting\n```markdown\n## Backend Development Update - [Feature Name]\n\n### Progress Summary\n- Current Status: [In Progress/Completed/Blocked]\n- Completion: [X]% complete\n- Timeline: On track / [X] days behind / [X] days ahead\n\n### Completed This Period\n- [API endpoint implementation]\n- [Database schema changes]\n- [Business logic implementation]\n- [Security features added]\n\n### API Development\n- Endpoints Implemented: [X] of [Y]\n- Response Time (95th percentile): [X]ms\n- Test Coverage: [X]%\n- Documentation: [Complete/In Progress]\n\n### Database Changes\n- Schema Migrations: [X] applied\n- Query Performance: Average [X]ms\n- Index Optimization: [Completed/In Progress]\n- Data Integrity: [Verified/Testing]\n\n### Security Implementation\n- Authentication: [Implemented/Testing]\n- Authorization: [Configured/In Progress]\n- Data Encryption: [Active/Implementing]\n- Vulnerability Scan: [Clean/Issues Found]\n\n### Performance Metrics\n- API Response Time: [X]ms (Target: <200ms)\n- Database Query Time: [X]ms (Target: <100ms)\n- Concurrent User Support: [X] users\n- Resource Utilization: CPU [X]%, Memory [X]%\n\n### Next Steps\n- [Immediate development task]\n- [Integration requirement]\n- [Performance optimization]\n\n### Blockers/Dependencies\n- [External service dependency]\n- [Database configuration requirement]\n- [Infrastructure provisioning need]\n```\n\n## Error Handling and Problem Solving\n\n### Common Issue Categories\n1. **Performance Bottlenecks**: Database queries, API response times, resource utilization\n2. **Security Vulnerabilities**: Authentication issues, data exposure, injection attacks\n3. **Integration Challenges**: Third-party service integration, data format mismatches\n4. **Scalability Constraints**: Concurrent user limits, resource exhaustion\n5. **Data Consistency**: Transaction management, concurrent access, data integrity\n\n### Problem-Solving Approach\n1. **Issue Analysis**: Identify root cause using logs, metrics, and debugging tools\n2. **Impact Assessment**: Determine business impact and urgency level\n3. **Solution Research**: Investigate best practices and proven solutions\n4. **Implementation Planning**: Design solution with rollback capability\n5. **Testing Strategy**: Plan comprehensive testing approach\n6. **Monitoring Setup**: Implement monitoring to prevent recurrence\n\n### Escalation Criteria\n- **Performance Issues**: When optimization requires infrastructure changes\n- **Security Concerns**: Critical vulnerabilities or compliance violations\n- **Architecture Decisions**: Complex system design choices requiring senior input\n- **Data Loss Risk**: Any operations that could result in data loss\n- **Service Outages**: Issues affecting production availability\n\n## Advanced Implementation Patterns\n\n### Microservices Architecture\n- **Service Decomposition**: Break down monoliths into focused services\n- **API Gateway**: Implement centralized API management and routing\n- **Service Discovery**: Implement service registration and discovery\n- **Circuit Breaker**: Add resilience patterns for service communication\n- **Distributed Tracing**: Implement request tracing across services\n\n### Asynchronous Processing\n- **Message Queues**: Implement reliable background job processing\n- **Event-Driven Architecture**: Design event-based system communication\n- **Webhook Handling**: Implement secure webhook processing\n- **Batch Processing**: Design efficient bulk data processing\n- **Real-time Features**: Implement WebSocket or Server-Sent Events\n\n### Data Management Patterns\n- **CQRS**: Separate read and write operations for complex domains\n- **Event Sourcing**: Implement event-based data persistence\n- **Database Sharding**: Implement horizontal database partitioning\n- **Read Replicas**: Implement read scaling with database replicas\n- **Caching Strategies**: Implement multi-level caching for performance\n\n## Security Implementation\n\n### Authentication and Authorization\n- **JWT Implementation**: Secure token-based authentication\n- **OAuth Integration**: Third-party authentication provider integration\n- **Role-Based Access Control**: Implement granular permission systems\n- **Session Management**: Secure session handling and timeout\n- **Multi-Factor Authentication**: Support for MFA when required\n\n### Data Protection\n- **Encryption**: Implement encryption for sensitive data\n- **Input Validation**: Comprehensive server-side validation\n- **SQL Injection Prevention**: Use parameterized queries and ORMs\n- **XSS Prevention**: Proper output encoding and Content Security Policy\n- **CSRF Protection**: Implement Cross-Site Request Forgery protection\n\n### Security Monitoring\n- **Audit Logging**: Log security-relevant events\n- **Anomaly Detection**: Monitor for unusual access patterns\n- **Vulnerability Scanning**: Regular security assessments\n- **Penetration Testing**: Support security testing activities\n- **Compliance Reporting**: Generate compliance and audit reports\n\n## Performance Optimization\n\n### Database Optimization\n- **Query Optimization**: Analyze and optimize slow queries\n- **Index Strategy**: Implement optimal indexing for query patterns\n- **Connection Pooling**: Configure efficient database connection management\n- **Query Caching**: Implement query result caching where appropriate\n- **Database Monitoring**: Monitor query performance and resource usage\n\n### Application Performance\n- **Code Profiling**: Identify and optimize performance bottlenecks\n- **Memory Management**: Efficient memory usage and garbage collection\n- **Caching Implementation**: Multi-level caching for frequently accessed data\n- **Asynchronous Processing**: Non-blocking operations for improved throughput\n- **Resource Pooling**: Efficient resource utilization and connection management\n\n## Continuous Learning and Improvement\n\n### Stay Current With\n- **Framework Updates**: Latest versions and best practices for chosen technologies\n- **Security Trends**: New vulnerability types and prevention techniques\n- **Performance Optimization**: New optimization techniques and tools\n- **Architecture Patterns**: Emerging architectural patterns and practices\n- **Database Technologies**: New database features and optimization techniques\n\n### Knowledge Sharing\n- Document architectural decisions and trade-offs\n- Share performance optimization techniques and results\n- Contribute to internal API design standards\n- Mentor junior developers on backend best practices\n- Present learnings from complex implementation challenges\n\n## Success Criteria\n\n### Quality Metrics\n- **Code Quality**: Static analysis score >95%, clean code principles followed\n- **Test Coverage**: >90% coverage for business logic, >80% overall\n- **Security Score**: Zero critical vulnerabilities, security best practices followed\n- **Performance Targets**: API response time <200ms, database queries <100ms\n- **Documentation Quality**: Complete API docs, clear deployment guides\n\n### System Reliability\n- **Uptime**: 99.9% availability target\n- **Error Rate**: <1% of requests result in errors\n- **Recovery Time**: <30 minutes for service restoration\n- **Data Integrity**: Zero data loss incidents\n- **Security Incidents**: Zero critical security breaches\n\n### Development Efficiency\n- **Delivery Predictability**: 90% of estimates within 20% accuracy\n- **Code Reusability**: Modular, reusable service components\n- **Review Efficiency**: Average 1-2 review cycles per feature\n- **Integration Success**: Smooth integration with frontend and external services\n- **Collaboration Effectiveness**: Positive feedback from team members\n\nRemember: Your role is to build robust, secure, and scalable backend systems that provide reliable foundations for applications while maintaining high code quality and collaborating effectively with the development team.\n\n## File: 05_TEMPLATES/02_BUILD_EVIDENCE.yaml\n# Atlas Build Evidence Template v2.0\n# This YAML template provides a standardized format for capturing build evidence\n\nevidence_metadata:\n  id: \"EVD-{YYYYMMDD}-{HHmm}-build-{sequence}\"\n  type: \"build\"\n  version: \"2.0\"\n  phase: \"implementation\"\n  story_id: \"\"  # e.g., \"S001\"\n  sprint_id: \"\"  # e.g., \"SP-2024-10\"\n  created_at: \"\"  # ISO 8601 format\n  created_by: \"automated\"\n  tools_used: []  # e.g., [\"maven\", \"docker\", \"sonarqube\"]\n  environment: \"\"  # dev|staging|production\n  validation_status: \"pending\"  # pending|validated|rejected\n  retention_policy: \"6months\"\n\nbuild_info:\n  build_id: \"\"  # e.g., \"BLD-20241115-1030-main-342\"\n  trigger: \"\"  # commit|schedule|manual|release\n  branch: \"\"  # e.g., \"main\", \"feature/auth-fix\"\n  commit_sha: \"\"  # Full commit hash\n  commit_message: \"\"\n  author: \"\"\n  build_duration: \"\"  # HH:MM:SS format\n  build_agent: \"\"  # e.g., \"jenkins-agent-03\"\n  build_number: 0\n  previous_build_number: 0\n\nenvironment:\n  build_server: \"\"  # e.g., \"jenkins.company.com\"\n  os_version: \"\"  # e.g., \"Ubuntu 22.04\"\n  architecture: \"\"  # e.g., \"x86_64\"\n  timezone: \"\"  # e.g., \"UTC\"\n  workspace_path: \"\"\n\n  build_tools:\n    - name: \"\"  # e.g., \"maven\"\n      version: \"\"  # e.g., \"3.8.6\"\n      path: \"\"\n    - name: \"\"  # e.g., \"openjdk\"\n      version: \"\"  # e.g., \"17.0.5\"\n      path: \"\"\n\n  environment_variables:\n    JAVA_HOME: \"\"\n    MAVEN_OPTS: \"\"\n    BUILD_ENV: \"\"\n\n  dependencies_snapshot: \"\"  # Path to dependencies file\n\ncompilation:\n  status: \"\"  # success|failure|warning\n  compiler_warnings: 0\n  compiler_errors: 0\n  compilation_time: \"\"  # HH:MM:SS\n  output_size: \"\"  # e.g., \"45.2MB\"\n\n  by_module:\n    - module_name: \"\"\n      status: \"\"  # success|failure|warning\n      warnings: 0\n      errors: 0\n      compilation_time: \"\"\n      output_size: \"\"\n\n  compiler_details:\n    compiler_version: \"\"\n    optimization_level: \"\"\n    target_version: \"\"\n    source_encoding: \"\"\n\ntesting:\n  unit_tests:\n    executed: 0\n    passed: 0\n    failed: 0\n    skipped: 0\n    duration: \"\"  # HH:MM:SS\n    coverage: 0.0  # Percentage\n\n    failures:\n      - test_class: \"\"\n        test_method: \"\"\n        error_message: \"\"\n        stack_trace: \"\"\n\n    coverage_by_package:\n      - package: \"\"\n        line_coverage: 0.0\n        branch_coverage: 0.0\n        method_coverage: 0.0\n\n  integration_tests:\n    executed: 0\n    passed: 0\n    failed: 0\n    skipped: 0\n    duration: \"\"  # HH:MM:SS\n\n    failures:\n      - test_class: \"\"\n        test_method: \"\"\n        error_message: \"\"\n        stack_trace: \"\"\n\n  test_reports:\n    junit_xml: \"\"  # Path to JUnit XML report\n    coverage_html: \"\"  # Path to coverage HTML report\n    coverage_xml: \"\"  # Path to coverage XML report\n\nstatic_analysis:\n  sonarqube:\n    quality_gate: \"\"  # passed|failed\n    project_key: \"\"\n    analysis_id: \"\"\n\n    metrics:\n      bugs: 0\n      vulnerabilities: 0\n      code_smells: 0\n      technical_debt: \"\"  # e.g., \"2h 15m\"\n      duplication: 0.0  # Percentage\n      lines_of_code: 0\n      complexity: 0\n      maintainability_rating: \"\"  # A|B|C|D|E\n      reliability_rating: \"\"  # A|B|C|D|E\n      security_rating: \"\"  # A|B|C|D|E\n\n    issues:\n      blocker: 0\n      critical: 0\n      major: 0\n      minor: 0\n      info: 0\n\n    detailed_issues:\n      - severity: \"\"  # blocker|critical|major|minor|info\n        type: \"\"  # bug|vulnerability|code_smell\n        rule: \"\"\n        message: \"\"\n        file: \"\"\n        line: 0\n        debt: \"\"  # e.g., \"5min\"\n\n  checkstyle:\n    total_violations: 0\n    error_violations: 0\n    warning_violations: 0\n    info_violations: 0\n\n    violations_by_category:\n      - category: \"\"  # e.g., \"imports\", \"naming\"\n        count: 0\n        severity: \"\"\n\n  spotbugs:\n    total_bugs: 0\n    high_priority: 0\n    medium_priority: 0\n    low_priority: 0\n\n    bug_categories:\n      - category: \"\"  # e.g., \"CORRECTNESS\", \"PERFORMANCE\"\n        count: 0\n        priority: \"\"\n\nsecurity_scanning:\n  dependency_check:\n    total_dependencies: 0\n    vulnerable_dependencies: 0\n\n    vulnerabilities:\n      critical: 0\n      high: 0\n      medium: 0\n      low: 0\n\n    detailed_vulnerabilities:\n      - component: \"\"\n        vulnerability: \"\"  # e.g., \"CVE-2023-12345\"\n        severity: \"\"  # critical|high|medium|low\n        description: \"\"\n        fix_available: false\n        fixed_version: \"\"\n\n  container_scan:\n    image_name: \"\"\n    image_tag: \"\"\n    scan_tool: \"\"  # e.g., \"trivy\", \"clair\"\n\n    vulnerabilities: 0\n    compliance_violations: 0\n\n    by_severity:\n      critical: 0\n      high: 0\n      medium: 0\n      low: 0\n      unknown: 0\n\n    base_image_vulnerabilities: 0\n    added_vulnerabilities: 0\n\n  license_check:\n    total_licenses: 0\n    approved_licenses: 0\n    unapproved_licenses: 0\n    unknown_licenses: 0\n\n    license_violations:\n      - component: \"\"\n        license: \"\"\n        violation_type: \"\"  # copyleft|restricted|unknown\n        action_required: \"\"\n\nartifacts:\n  - name: \"\"  # e.g., \"user-service.jar\"\n    type: \"\"  # jar|war|docker|npm|wheel\n    size: \"\"  # e.g., \"34.5MB\"\n    checksum: \"\"  # e.g., \"sha256:a1b2c3...\"\n    location: \"\"  # Full path or URL\n    created_at: \"\"  # ISO 8601 format\n\n  - name: \"\"  # Docker image example\n    type: \"docker\"\n    tag: \"\"  # e.g., \"user-service:2.1.0-rc.12\"\n    size: \"\"  # e.g., \"156MB\"\n    digest: \"\"  # sha256 digest\n    registry: \"\"  # e.g., \"registry.company.com\"\n    layers: 0\n    created_at: \"\"\n\ndeployment_readiness:\n  health_checks: \"\"  # passed|failed\n  configuration_validation: \"\"  # passed|failed\n  database_migrations: \"\"  # ready|pending|failed\n  feature_flags: \"\"  # configured|missing\n  secrets_validation: \"\"  # passed|failed\n\n  readiness_checklist:\n    - item: \"Application starts successfully\"\n      status: \"\"  # passed|failed|skipped\n      evidence: \"\"\n    - item: \"Database connectivity verified\"\n      status: \"\"\n      evidence: \"\"\n    - item: \"External service dependencies verified\"\n      status: \"\"\n      evidence: \"\"\n    - item: \"Monitoring endpoints responsive\"\n      status: \"\"\n      evidence: \"\"\n\nperformance_metrics:\n  build_performance:\n    total_build_time: \"\"  # HH:MM:SS\n    compilation_time: \"\"\n    test_execution_time: \"\"\n    packaging_time: \"\"\n    analysis_time: \"\"\n\n  resource_usage:\n    peak_memory_usage: \"\"  # e.g., \"2.1GB\"\n    average_cpu_usage: 0.0  # Percentage\n    disk_space_used: \"\"  # e.g., \"1.5GB\"\n    network_usage: \"\"  # e.g., \"150MB\"\n\n  historical_comparison:\n    previous_build_time: \"\"\n    performance_delta: \"\"  # +5% slower, -10% faster\n    trend: \"\"  # improving|declining|stable\n\nnotifications:\n  sent_to: []  # e.g., [\"team-slack\", \"build-email-list\"]\n  notification_time: \"\"  # ISO 8601 format\n  status: \"\"  # success|failed|partial\n\n  channels:\n    - type: \"\"  # slack|email|webhook\n      target: \"\"  # channel name, email address, URL\n      status: \"\"  # sent|failed\n      message_id: \"\"\n\nrollback_information:\n  rollback_available: false\n  previous_successful_build: \"\"\n  rollback_procedure: \"\"\n  rollback_testing_required: false\n\n  rollback_artifacts:\n    - name: \"\"\n      location: \"\"\n      verified: false\n\nevidence_files:\n  - name: \"build-log-full.txt\"\n    path: \"\"\n    size: \"\"\n    checksum: \"\"\n    description: \"Complete build execution log\"\n\n  - name: \"test-results.xml\"\n    path: \"\"\n    size: \"\"\n    checksum: \"\"\n    description: \"JUnit test results in XML format\"\n\n  - name: \"sonarqube-report.json\"\n    path: \"\"\n    size: \"\"\n    checksum: \"\"\n    description: \"SonarQube analysis results\"\n\n  - name: \"dependency-check-report.xml\"\n    path: \"\"\n    size: \"\"\n    checksum: \"\"\n    description: \"OWASP dependency check results\"\n\n  - name: \"coverage-report/\"\n    path: \"\"\n    size: \"\"\n    checksum: \"\"\n    description: \"Test coverage HTML reports\"\n\nvalidation_rules:\n  critical:\n    - compilation_status == \"success\"\n    - unit_test_failures == 0\n    - security_critical_issues == 0\n\n  blocking:\n    - integration_test_failures == 0\n    - quality_gate == \"passed\"\n    - artifacts_generated == true\n    - deployment_readiness == \"passed\"\n\n  warning:\n    - test_coverage >= 80\n    - build_duration <= \"00:05:00\"\n    - static_analysis_violations < 20\n    - performance_regression < 10\n\nquality_gates:\n  minimum_requirements:\n    test_coverage: 80.0\n    quality_gate: \"passed\"\n    security_violations: 0\n    build_success: true\n\n  performance_requirements:\n    max_build_time: \"00:05:00\"\n    max_memory_usage: \"4GB\"\n    max_artifact_size: \"100MB\"\n\n  compliance_requirements:\n    license_compliance: true\n    security_scan_passed: true\n    code_standards_met: true\n\n# Usage Instructions:\n# 1. Replace placeholder values (empty strings, zeros) with actual data\n# 2. Remove unused sections if they don't apply to your build type\n# 3. Add custom fields as needed for your specific requirements\n# 4. Ensure all paths and URLs are accessible for evidence verification\n# 5. Validate the YAML structure before committing to evidence store\n\n## Dependency: testing_standards\n# Atlas Velocity Tracking System v2.0\n\n## Overview\n\nThe Atlas Velocity Tracking System provides comprehensive metrics to measure and optimize team performance across multiple dimensions. This system goes beyond simple story point tracking to include quality-adjusted velocity, predictability metrics, and continuous improvement indicators.\n\n## Core Velocity Metrics\n\n### 1. Traditional Velocity Metrics\n\n#### Story Point Velocity\n**Definition**: Story points completed per sprint\n**Calculation**: Sum of story points for all completed stories in a sprint\n**Target**: Maintain consistent velocity \u00b115% sprint-over-sprint\n\n```\nSprint Velocity = \u03a3(Completed Story Points)\nRolling Average Velocity = (Last 6 Sprints Velocity) / 6\n```\n\n#### Feature Delivery Rate\n**Definition**: Number of features delivered per sprint\n**Calculation**: Count of features marked as \"Done\" in sprint\n**Target**: Minimum 2 features per sprint, trending upward\n\n```\nFeature Delivery Rate = Count(Completed Features) / Sprint Duration\n```\n\n#### Cycle Time\n**Definition**: Time from story creation to production deployment\n**Calculation**: Average time across all completed stories\n**Target**: <14 days for standard features, <7 days for small features\n\n```\nCycle Time = Deployment Date - Story Creation Date\nAverage Cycle Time = \u03a3(Individual Cycle Times) / Count(Stories)\n```\n\n### 2. Quality-Adjusted Velocity Metrics\n\n#### Quality-Weighted Velocity\n**Definition**: Story points adjusted for quality score\n**Calculation**: Story points \u00d7 (Quality Score / 100)\n**Target**: Quality-weighted velocity \u2265 85% of raw velocity\n\n```\nQuality-Weighted Velocity = \u03a3(Story Points \u00d7 Quality Score / 100)\nQuality Adjustment Factor = Quality-Weighted Velocity / Raw Velocity\n```\n\n#### Defect-Adjusted Velocity\n**Definition**: Story points reduced by defect rework impact\n**Calculation**: Original velocity minus defect remediation effort\n**Target**: Defect impact <10% of total velocity\n\n```\nDefect Impact = \u03a3(Defect Fix Story Points)\nDefect-Adjusted Velocity = Sprint Velocity - Defect Impact\nDefect Rate = Defect Impact / Sprint Velocity\n```\n\n#### First-Time-Right Velocity\n**Definition**: Story points that required no rework or defect fixes\n**Calculation**: Velocity from stories with zero post-completion issues\n**Target**: >90% of delivered story points should be first-time-right\n\n```\nFirst-Time-Right Velocity = \u03a3(Zero-Rework Story Points)\nFirst-Time-Right Rate = First-Time-Right Velocity / Total Velocity\n```\n\n### 3. Predictability Metrics\n\n#### Velocity Variance\n**Definition**: Standard deviation of sprint velocities\n**Calculation**: Statistical variance across recent sprints\n**Target**: Coefficient of variation <20%\n\n```\nVelocity Variance = \u03c3(Sprint Velocities)\nCoefficient of Variation = (Velocity Variance / Mean Velocity) \u00d7 100\n```\n\n#### Commitment Reliability\n**Definition**: Percentage of sprint commitments successfully delivered\n**Calculation**: Completed story points / Committed story points\n**Target**: >85% commitment reliability\n\n```\nCommitment Reliability = (Completed Points / Committed Points) \u00d7 100\nSprint Success Rate = Sprints with >85% Completion / Total Sprints\n```\n\n#### Forecast Accuracy\n**Definition**: Accuracy of velocity-based delivery predictions\n**Calculation**: Predicted vs. actual delivery dates\n**Target**: <10% variance in delivery predictions\n\n```\nForecast Error = |Predicted Date - Actual Date| / Predicted Duration\nForecast Accuracy = (1 - Average Forecast Error) \u00d7 100\n```\n\n### 4. Flow Metrics\n\n#### Work in Progress (WIP)\n**Definition**: Number of stories actively being worked\n**Calculation**: Count of stories in \"In Progress\" states\n**Target**: WIP limit based on team size (typically 1.5 \u00d7 team size)\n\n```\nCurrent WIP = Count(Stories in Progress)\nWIP Utilization = Current WIP / WIP Limit\n```\n\n#### Throughput\n**Definition**: Number of stories completed per time period\n**Calculation**: Stories completed / time period\n**Target**: Consistent throughput aligned with capacity\n\n```\nWeekly Throughput = Stories Completed / Week\nMonthly Throughput = Stories Completed / Month\n```\n\n#### Flow Efficiency\n**Definition**: Percentage of cycle time spent on value-adding work\n**Calculation**: Active work time / total cycle time\n**Target**: >25% flow efficiency\n\n```\nFlow Efficiency = (Active Work Time / Total Cycle Time) \u00d7 100\nWait Time = Total Cycle Time - Active Work Time\n```\n\n## Advanced Velocity Analytics\n\n### 1. Velocity Decomposition Analysis\n\n#### By Story Type\nTrack velocity contribution by story type:\n- **New Features**: 60-70% of velocity\n- **Bug Fixes**: <15% of velocity\n- **Technical Debt**: 10-20% of velocity\n- **Infrastructure**: 5-15% of velocity\n\n#### By Team Member\nIndividual contribution analysis:\n- Velocity per developer\n- Specialization impact\n- Cross-training opportunities\n- Capacity utilization\n\n#### By Epic/Component\nSystem-level velocity tracking:\n- Component delivery rates\n- Epic completion trends\n- Architecture impact on velocity\n\n### 2. Velocity Trend Analysis\n\n#### Seasonal Patterns\n- Holiday impact analysis\n- Training period effects\n- Onboarding ramp-up curves\n- Release preparation impacts\n\n#### Capacity Changes\n- Team size impact\n- New team member integration\n- Knowledge transfer effects\n- Tool and process changes\n\n#### External Dependencies\n- Third-party integration delays\n- Infrastructure bottlenecks\n- Cross-team dependency impacts\n- Customer feedback incorporation\n\n## Velocity Improvement Strategies\n\n### 1. Bottleneck Identification\n\n#### Process Bottlenecks\n- Code review delays\n- Testing resource constraints\n- Deployment pipeline issues\n- Requirements clarification delays\n\n#### Technical Bottlenecks\n- Complex legacy code areas\n- Performance optimization needs\n- Testing environment limitations\n- Tool performance issues\n\n#### Team Bottlenecks\n- Skill gaps in specific areas\n- Communication inefficiencies\n- Decision-making delays\n- Knowledge silos\n\n### 2. Velocity Optimization Techniques\n\n#### Sprint Planning Optimization\n- Better story sizing consistency\n- Improved capacity planning\n- Dependency identification\n- Risk assessment integration\n\n#### Work Breakdown Improvement\n- Smaller, more predictable stories\n- Better acceptance criteria\n- Technical spike identification\n- Cross-cutting concern planning\n\n#### Flow Optimization\n- WIP limit enforcement\n- Batch size reduction\n- Context switching minimization\n- Parallel work stream design\n\n## Velocity Reporting and Dashboards\n\n### 1. Sprint-Level Reports\n\n#### Sprint Velocity Report\n```\nSprint 24 Velocity Summary\n========================\nRaw Velocity: 42 points\nQuality-Adjusted: 38 points (90%)\nCommitment: 40 points\nReliability: 95%\n\nStory Breakdown:\n- Features: 28 points (67%)\n- Bugs: 6 points (14%)\n- Tech Debt: 8 points (19%)\n\nQuality Metrics:\n- Average Quality Score: 85/100\n- First-Time-Right: 36 points (86%)\n- Defect Impact: 2 points (5%)\n```\n\n#### Trend Analysis\n```\n6-Sprint Rolling Metrics\n=======================\nAverage Velocity: 39 \u00b1 4 points\nVelocity Trend: +5% (improving)\nCommitment Reliability: 88%\nQuality Trend: +12% (improving)\n\nBottleneck Analysis:\n1. Code Review: 2.1 days avg\n2. Testing: 1.8 days avg\n3. Requirements: 1.2 days avg\n```\n\n### 2. Release-Level Reports\n\n#### Release Velocity Summary\n```\nRelease 2.1 Summary\n==================\nDuration: 6 sprints\nTotal Velocity: 234 points\nFeatures Delivered: 18\nAverage Quality Score: 87/100\n\nKey Achievements:\n- 12% velocity improvement\n- 25% reduction in cycle time\n- 95% commitment reliability\n- Zero critical production defects\n```\n\n### 3. Velocity Dashboard Components\n\n#### Real-Time Metrics\n- Current sprint progress\n- Daily velocity burn-down\n- WIP limits and utilization\n- Blocked story count\n\n#### Trend Visualizations\n- Velocity trend charts\n- Quality score trends\n- Cycle time trends\n- Predictability metrics\n\n#### Comparative Analytics\n- Team-to-team velocity comparison\n- Project-to-project analysis\n- Historical performance comparison\n- Industry benchmark comparisons\n\n## Velocity Data Collection\n\n### 1. Automated Data Sources\n\n#### Project Management Tools\n- Story point tracking\n- Sprint completion data\n- Cycle time measurements\n- Work item state changes\n\n#### Development Tools\n- Code commit frequency\n- Pull request metrics\n- Build and deployment data\n- Code review timings\n\n#### Quality Systems\n- Test coverage data\n- Defect tracking\n- Quality score calculations\n- User feedback metrics\n\n### 2. Manual Data Collection\n\n#### Sprint Retrospectives\n- Team satisfaction scores\n- Process improvement ideas\n- Bottleneck identification\n- Capacity planning insights\n\n#### Stakeholder Feedback\n- Business value delivery\n- Feature adoption rates\n- Customer satisfaction\n- Market response metrics\n\n## Velocity Forecasting\n\n### 1. Predictive Models\n\n#### Simple Velocity Forecasting\n```python\n# Rolling average prediction\ndef predict_velocity(historical_velocities, periods=3):\n    return sum(historical_velocities[-periods:]) / periods\n\n# Trend-based prediction\ndef predict_with_trend(velocities):\n    trend = calculate_trend(velocities)\n    latest = velocities[-1]\n    return latest + trend\n```\n\n#### Monte Carlo Simulation\n```python\n# Probabilistic velocity forecasting\ndef monte_carlo_forecast(velocities, simulations=1000):\n    # Generate probability distribution\n    mean_velocity = np.mean(velocities)\n    std_velocity = np.std(velocities)\n\n    # Run simulations\n    forecasts = np.random.normal(mean_velocity, std_velocity, simulations)\n\n    return {\n        '50th_percentile': np.percentile(forecasts, 50),\n        '80th_percentile': np.percentile(forecasts, 80),\n        '90th_percentile': np.percentile(forecasts, 90)\n    }\n```\n\n### 2. Release Planning\n\n#### Feature-Based Forecasting\n- Epic-level story point estimation\n- Dependency-adjusted timelines\n- Risk-buffered delivery dates\n- Scope flexibility planning\n\n#### Capacity-Based Planning\n- Team availability forecasting\n- Holiday and training impact\n- Skill development time allocation\n- External dependency coordination\n\n## Velocity Improvement Process\n\n### 1. Weekly Velocity Reviews\n\n#### Review Agenda\n1. Current sprint velocity progress\n2. Bottleneck identification and resolution\n3. Quality impact assessment\n4. Process improvement opportunities\n5. Next sprint capacity planning\n\n#### Action Items\n- Immediate bottleneck removal\n- Process adjustments\n- Tool improvements\n- Skill development needs\n\n### 2. Monthly Velocity Retrospectives\n\n#### Deep Analysis\n- Velocity trend root cause analysis\n- Quality vs. velocity trade-off review\n- Team capacity optimization opportunities\n- Long-term improvement planning\n\n#### Strategic Planning\n- Velocity targets for next quarter\n- Process enhancement roadmap\n- Tool and infrastructure investments\n- Team development plans\n\n## Success Criteria\n\nThe velocity tracking system is successful when:\n\n1. **Predictability Improves**: Forecast accuracy >90%, commitment reliability >85%\n2. **Quality Maintains**: Quality-adjusted velocity \u226585% of raw velocity\n3. **Continuous Improvement**: 5% velocity improvement per quarter\n4. **Sustainable Pace**: Low velocity variance (<20% CV), team satisfaction >4/5\n5. **Business Value**: Feature delivery rate increases, customer satisfaction improves\n\n## Integration with Atlas Framework\n\n### Script Integration\n```bash\n# Calculate current sprint velocity\npython3 velocity_tracker.py current --sprint S2023-24\n\n# Generate velocity report\npython3 velocity_tracker.py report --period 6months --format detailed\n\n# Forecast future velocity\npython3 velocity_tracker.py forecast --method monte_carlo --confidence 80\n```\n\n### Quality Score Integration\n```bash\n# Calculate quality-adjusted velocity\npython3 velocity_tracker.py quality_adjusted --sprint S2023-24\n\n# Track velocity vs quality trends\npython3 velocity_tracker.py trends --include quality --period 1year\n```\n\nThis comprehensive velocity tracking system provides the data and insights needed to continuously improve team performance while maintaining high quality standards.\n\n\n## \u2705 Verification Checklist\n**Backend Development Checklist**\n_API and service implementation checks_\n\n### Required Checks:\n- [ ] API follows RESTful principles\n- [ ] Input validation implemented\n- [ ] Comprehensive error handling\n- [ ] Authentication/authorization implemented\n- [ ] Database queries optimized\n- [ ] Structured logging implemented\n- [ ] API documentation updated\n- [ ] Unit tests >90% coverage\n\n### Optional Checks:\n- [ ] Caching strategy implemented where appropriate\n- [ ] Rate limiting configured\n- [ ] Monitoring/metrics added\n\n_Complete 8 required checks before proceeding_", "metadata": {"task": "backend_development", "feature": "authentication", "timestamp": "2025-09-20T10:16:50.209152", "files_included": [{"path": "03_AGENTS/backend_developer_prompt.md", "size": 16515, "priority": 1}, {"path": "05_TEMPLATES/02_BUILD_EVIDENCE.yaml", "size": 8858, "priority": 2}], "files_excluded": [], "dependencies_resolved": [{"name": "testing_standards", "files": ["04_METRICS/02_VELOCITY_TRACKING.md"]}], "total_tokens": 9521, "cache_key": "be84d897aa9ac36aadd172a4b2473d59", "checklist_included": true, "total_size": 38085}}